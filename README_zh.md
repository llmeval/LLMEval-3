<div align="center">
<h2>LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models</h2>

[![Paper](https://img.shields.io/badge/Paper-Arxiv-blue.svg?style=for-the-badge)](https://arxiv.org/abs/2508.05452)


</div>

<div align="center">
<img src=".\pic\llmeval-logo.png" alt="llmeval-logo" style="zoom:50%;" />
</div>

> **注意：** 英文版 README 请参阅 [README.md](README.md)。

## 📚 题目内容与形式

LLMEval-3聚焦于专业知识能力评测，涵盖哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事学、管理学、艺术学等教育部划定的13个学科门类、50余个二级学科，共计约20W道标准生成式问答题目（后续我们将继续收集题目将总题库扩充至100W）。



<div align="center">
<img src=".\pic\subjects.PNG" alt="学科覆盖范围" style="zoom:80%;" />
</div>

题目来源主要包括**大学本科课后作业**、**大学本科期中期末考试**、**研究生入学考试**等。为了尽可能的防止参与评测的大模型在预训练阶段引入大比例原始评测数据，LLMEval-3评测题目来源尽可能为非互联网公开渠道，数据格式为PDF和Word文件，经过一定的OCR识别与数据清洗之后，将题目进行格式化处理。针对于不同的题型，提供给待测试模型标准接口，实现全流程自动化。

与其他知识评测所采用的选择题模式不同，LLMEval-3中所有问题将统一处理为**生成式知识问答**形式，并尽可能包含多种题型，包括简答，计算、判断、辨析、写作等。相较于具有标准格式的选择题，LLMEval-3所采用的生成式知识问答，能够更好地反映用户实际需求以及模型语言能力。

## 🔬 评测方法

### 评测流程

防止作弊是LLMEval-3考虑的重要因素。现有公开评测基准存在测试题库泄露的问题，因此可能出现“刷榜”、“刷分”等不公平现象，在LLMEval-3中，每个参与评测的系统需要完成从总题库中随机抽样的1000题，**针对同一机构的模型，确保每次评测题目不重复**。评测过程将采用在线方式，一轮评测中题目的发送串行进行，即下一题的发送将会视上一道题目的回答情况而定，避免恶意爬取行为。

本轮评测采用自动评测方法打分，现在使用的评测模型是GPT4 Turbo。每道题得分的范围为0-3分。评分聚焦于回答的核心正确性和解释正确性，其中核心正确性作为衡量分数的主要指标。采用评测prompt如下所示：

```text
Please evaluate the following response from the LLM regarding a discipline-specific question based on the following criteria. You must score it on a scale of 0, 1, 2 or 3 stars:

Overall Rating:
0 stars indicate wrong answer with a wrong explanation
1 star indicates wrong answer but a partially reasonable explanation
2 stars indicate a correct answer with a partially reasonable explanation
3 stars indicate a correct answer with a reasonable explanation

User: {question}

LLM:{answer_from_llm}

The correct answer to user's question is: {correct_answer}

You must provide your feedback in the following format:
{"Overall Rating":numbers of its stars(int)}
```

### 评分方法

为了规避由随机抽样1000题引入的系统偏差，LLMEval-3使用**相对分数**和**绝对分数**两个指标。

**相对分数计算：**
考虑到大语言模型技术的快速发展，我们引入相对分数来衡量模型与当前最佳性能的差距。我们选择榜单中表现最优的模型作为SOTA基准，目前为Doubao-1.5-Thinking-Pro：

$$R_{\text{SOTA}}^{\text{model}}=\frac{S_{model}}{S_\text{sotamodel}} \times 100 $$



**绝对分数计算：**
绝对分数表示模型在N=1000道题目上的原始表现，计算方式为将每题得分（0-3分）归一化到0-100区间：

$$S_{model}=\sum_{i=1}^N{\frac{s_i}{s_{max}} \times 100} \quad (1)$$

其中 $s_i$ 为第i题得分， $s_{max}=3$ 。

**评分说明：** $S_{\text{model}}$为绝对分数（0-100量表）， $R_{\text{SOTA}}^{\text{model}}$ 为相对分数（以SOTA模型为100%基准），各学科分数采用10分制量表。


## 🏆 当前排行榜（截至2025年8月）

### 📋 总体分数

| 模型名称 | 发布机构 | 访问方式 | 发布日期 | 相对分数 | 绝对分数 |
|----------|----------|----------|----------|----------|----------|
| Doubao-1.5-Thinking-Pro | 字节跳动 | API | 2025.4.15 | 100.00 | 93.67 |
| DeepSeek-R1 | DeepSeek | API | 2025.5.28 | 97.40 | 91.23 |
| Gemini-2.5-Pro-Preview | Google | API | 2025.6.5 | 97.22 | 91.07 |
| Gemini-2.5-Pro-Preview-Thinking | Google | API | 2025.6.5 | 97.15 | 91.00 |
| DeepSeek-V3 | DeepSeek | API | 2025.3.24 | 96.48 | 90.37 |
| Qwen3-235B | Alibaba Cloud | API | 2025.4.29 | 96.44 | 90.33 |
| Doubao-1.5-Pro-256K | 字节跳动 | API | 2025.1.15 | 95.69 | 89.63 |
| QwQ-32B | Alibaba Cloud | API | 2025.3.6 | 94.52 | 88.54 |
| GPT-5 | OpenAI | API | 2025.8.7 | 93.85 | 87.9 |
| O1-2024-12-17 | OpenAI | API | 2024.12.17 | 93.35 | 87.43 |
| Gemini-2.5-Flash-Thinking | Google | API | 2025.4.17 | 92.74 | 86.87 |
| Qwen3-32B | Alibaba Cloud | API | 2025.4.29 | 92.21 | 86.37 |
| Claude-Sonnet-4-Thinking | Anthropic | API | 2025.5.14 | 91.03 | 85.27 |
| Claude-Sonnet-4 | Anthropic | API | 2025.5.14 | 91.00 | 85.23 |
| GPT-4o-Search-Preview | OpenAI | API | 2024.11.20 | 89.40 | 83.73 |
| GLM-4-32B | Tsinghua&Zhipu.AI | API | 2025.4.14 | 88.43 | 82.83 |
| GPT-4o-2024-11-20 | OpenAI | API | 2024.11.20 | 88.08 | 82.50 |
| Gemini-1.5-Pro | Google | API | 2024.2.14 | 85.92 | 80.47 |
| Qwen2.5-32B-Instruct | Alibaba Cloud | API | 2024.9.19 | 85.07 | 79.68 |
| O3-Mini | OpenAI | API | 2025.1.29 | 84.13 | 78.80 |
| Qwen-Turbo-1101 | Alibaba Cloud | API | 2024.11.1 | 83.71 | 78.41 |
| Claude-3.5-Sonnet | Anthropic | API | 2024.10.22 | 83.38 | 78.10 |
| O1-Mini-2024-09-12 | OpenAI | API | 2024.9.12 | 78.93 | 73.93 |
| Claude-3-Haiku | Anthropic | API | 2024.3.7 | 62.95 | 58.97 |
| LLaMA-3.2-90B-Vision-Instruct | Meta | API | 2024.9.25 | 61.74 | 57.83 |
| LLaMA-3.3-70B | Meta | API | 2024.12.6 | 60.85 | 57.00 |
| Phi-3-Medium-128K-Instruct | Microsoft | API | 2024.5.3 | 36.94 | 34.60 |
| GPT-4 Turbo(gpt-4-1106-preview) | OpenAI | API | 2023.11.6 | 78.56 | 73.6 |
| GPT-4-0125-Preview | OpenAI | API | 2024.1.26 | 76.44 | 71.6 |
| 百度文心4.0 | 百度 | API | 2023.10.17 | 75.09 | 70.33 |
| Yi-34B-Chat | 零一万物 | API | 2023.11.24 | 70.17 | 65.70 |
| 百度文心3.5 | 百度 | API | 2023.7.6 | 69.14 | 64.73 |
| ChatGLM-Pro | 清华&智谱AI | API | 2023.9.25 | 69.14 | 64.73 |
| GPT-4-0613 | OpenAI | API | 2023.6.13 | 66.17 | 61.97 |
| 讯飞星火v3.0 | 科大讯飞 | API | 2023.10.24 | 65.64 | 61.47 |
| Nanbeige-Plus | 南北阁实验室 | API | 2023.12.1 | 65.14 | 61.00 |
| Baichuan2-13B-Chat | 百川智能 | 权重 | 2023.9.6 | 58.31 | 54.6 |
| Gemini-Pro | Google | API | 2023.12.13 | 58.20 | 54.5 |
| Qwen-Plus | 阿里云 | API | 2023.11.1 | 56.60 | 53.0 |
| Qwen-Turbo | 阿里云 | API | 2023.9.1 | 55.78 | 52.23 |
| Nanbeige-16B | 南北阁实验室 | API | 2023.11.19 | 55.46 | 51.93 |
| GPT-3.5-Turbo | OpenAI | API | 2023.6.13 | 55.42 | 51.9 |
| MiniMax-Abab5 | MiniMax | 权重 | 2023.8.31 | 55.33 | 51.83 |
| Mixtral-8x7B-Instruct | Mistral AI | 权重 | 2023.12.11 | 51.69 | 48.4 |
| ChatGLM2-6B | 清华&智谱AI | 权重 | 2023.6.25 | 42.32 | 39.63 |
| Ziya-v1.1-13B | IDEA研究院 | 权重 | 2023.6.7 | 40.18 | 37.63 |
| InternLM-Chat-7B | 上海AI实验室&商汤 | 权重 | 2023.7.6 | 38.73 | 36.27 |
| Linly-Chinese-LLaMA-2-13B-HF | 大数据系统计算技术国家工程实验室 | 权重 | 2023.7.25 | 37.06 | 34.7 |
| BELLE-LLaMA2-13B-Chat-0.4M | 链家科技 | 权重 | 2023.7.6 | 36.28 | 33.97 |
| LLaMA-2-7B-Chat-HF | Meta | 权重 | 2023.7.18 | 25.24 | 23.63 |

### 📊 学科专业表现

| 模型名称 | 总分 | 工学 | 经济学 | 教育学 | 法学 | 文学 | 管理学 | 理学 | 历史学 | 医学 | 军事学 |
|----------|------|------|--------|--------|------|------|--------|------|--------|------|--------|
| Doubao-1.5-Thinking-Pro | 93.67 | 9.47 | 9.67 | 9.43 | 9.77 | 8.93 | 9.53 | 9.23 | 9.70 | 8.97 | 8.97 |
| DeepSeek-R1 | 91.23 | 9.47 | 9.43 | 9.27 | 9.37 | 8.83 | 9.37 | 9.03 | 9.53 | 8.50 | 8.43 |
| Gemini-2.5-Pro-Preview | 91.07 | 9.20 | 9.47 | 9.20 | 9.30 | 8.43 | 9.63 | 9.07 | 9.40 | 8.50 | 8.87 |
| Gemini-2.5-Pro-Preview-Thinking | 91.00 | 9.13 | 9.50 | 9.37 | 9.47 | 8.40 | 9.63 | 9.20 | 9.27 | 8.30 | 8.73 |
| DeepSeek-V3 | 90.37 | 9.30 | 9.57 | 8.93 | 9.23 | 8.60 | 9.13 | 8.97 | 9.47 | 8.83 | 8.33 |
| Qwen3-235B | 90.33 | 9.23 | 9.43 | 9.03 | 9.50 | 8.23 | 9.43 | 8.97 | 9.17 | 8.73 | 8.60 |
| Doubao-1.5-Pro-256K | 89.63 | 8.83 | 9.03 | 9.13 | 9.43 | 8.57 | 9.27 | 8.83 | 9.10 | 8.60 | 8.83 |
| QwQ-32B | 88.54 | 8.30 | 9.46 | 9.23 | 9.33 | 7.83 | 9.46 | 8.65 | 9.27 | 8.57 | 8.43 |
| GPT-5 | 87.9 | 8.83 | 9.37 | 8.90 | 8.87 | 8.10 | 9.10 | 8.90 | 9.03 | 8.50 | 8.30 |
| O1-2024-12-17 | 87.43 | 8.90 | 9.30 | 8.67 | 8.77 | 7.73 | 9.27 | 8.90 | 8.97 | 8.17 | 8.77 |
| Gemini-2.5-Flash-Thinking | 86.87 | 8.67 | 9.27 | 8.70 | 9.00 | 7.80 | 8.93 | 8.90 | 9.00 | 8.03 | 8.57 |
| Qwen3-32B | 86.37 | 8.43 | 9.10 | 8.57 | 9.10 | 7.77 | 9.47 | 8.67 | 9.30 | 7.70 | 8.27 |
| Claude-Sonnet-4-Thinking | 85.27 | 8.57 | 9.00 | 8.63 | 8.73 | 7.57 | 9.10 | 8.93 | 8.70 | 7.97 | 8.07 |
| Claude-Sonnet-4 | 85.23 | 8.57 | 8.80 | 8.50 | 8.70 | 7.80 | 9.03 | 8.80 | 8.80 | 8.17 | 8.07 |
| GPT-4o-Search-Preview | 83.73 | 8.27 | 8.77 | 8.43 | 8.67 | 7.77 | 8.80 | 8.20 | 8.73 | 8.27 | 7.83 |
| GLM-4-32B | 82.83 | 7.77 | 8.97 | 8.33 | 8.33 | 7.03 | 9.13 | 8.27 | 8.77 | 8.23 | 8.00 |
| GPT-4o-2024-11-20 | 82.50 | 7.90 | 8.67 | 8.30 | 8.33 | 7.17 | 8.97 | 8.57 | 8.67 | 7.63 | 8.30 |
| Gemini-1.5-Pro | 80.47 | 8.13 | 8.45 | 8.30 | 8.37 | 7.04 | 8.17 | 8.43 | 8.50 | 7.48 | 7.60 |
| Qwen2.5-32B-Instruct | 79.68 | 7.70 | 8.57 | 8.33 | 8.33 | 6.70 | 8.50 | 8.17 | 7.70 | 7.60 | 8.08 |
| O3-Mini | 78.80 | 7.97 | 8.60 | 8.30 | 8.20 | 6.73 | 8.57 | 8.53 | 7.17 | 7.03 | 7.70 |
| Qwen-Turbo-1101 | 78.41 | 7.97 | 8.37 | 8.03 | 8.23 | 6.40 | 8.50 | 8.10 | 7.50 | 7.27 | 8.05 |
| Claude-3.5-Sonnet | 78.10 | 7.97 | 8.53 | 8.27 | 7.93 | 7.03 | 8.50 | 8.00 | 7.57 | 6.70 | 7.60 |
| O1-Mini-2024-09-12 | 73.93 | 7.27 | 8.43 | 7.90 | 7.53 | 6.27 | 8.27 | 8.17 | 6.43 | 6.63 | 7.03 |
| Claude-3-Haiku | 58.97 | 5.80 | 6.60 | 6.97 | 6.63 | 4.83 | 5.93 | 6.33 | 4.80 | 5.23 | 5.83 |
| LLaMA-3.2-90B-Vision-Instruct | 57.83 | 5.63 | 6.33 | 6.20 | 5.80 | 4.73 | 6.10 | 6.57 | 5.03 | 5.27 | 6.17 |
| LLaMA-3.3-70B | 57.00 | 5.80 | 6.90 | 5.63 | 5.70 | 5.47 | 5.70 | 6.30 | 4.70 | 4.87 | 5.93 |
| Phi-3-Medium-128K-Instruct | 34.60 | 2.27 | 4.17 | 3.70 | 4.23 | 2.87 | 4.50 | 3.57 | 3.20 | 2.27 | 3.83 |
| GPT-4 Turbo(gpt-4-1106-preview) | 73.6 | 6.97 | 8.17 | 8.33 | 7.8 | 6.0 | 7.57 | 8.13 | 7.0 | 6.43 | 7.2 |
| GPT-4-0125-Preview | 71.6 | 6.9 | 7.4 | 8.03 | 7.3 | 6.0 | 7.47 | 7.63 | 6.87 | 6.33 | 7.67 |
| 百度文心4.0 | 70.33 | 7.27 | 7.23 | 7.67 | 7.43 | 5.63 | 6.47 | 6.8 | 7.63 | 7.8 | 6.4 |
| Yi-34B-Chat | 65.70 | 5.77 | 6.63 | 7.37 | 7.53 | 5.47 | 5.77 | 5.47 | 7.47 | 6.3 | 7.93 |
| 百度文心3.5 | 64.73 | 6.2 | 6.7 | 7.8 | 6.83 | 5.2 | 5.5 | 6.0 | 7.23 | 6.57 | 6.7 |
| ChatGLM-Pro | 64.73 | 5.9 | 7.07 | 7.03 | 7.9 | 5.43 | 6.33 | 5.0 | 6.67 | 5.97 | 7.43 |
| GPT-4-0613 | 61.97 | 6.5 | 6.73 | 6.6 | 6.73 | 5.43 | 6.1 | 6.47 | 5.3 | 5.2 | 6.9 |
| 讯飞星火v3.0 | 61.47 | 5.77 | 6.5 | 7.27 | 7.3 | 5.7 | 5.9 | 5.03 | 6.5 | 5.23 | 6.27 |
| Nanbeige-Plus | 61.00 | 5.78 | 5.57 | 6.77 | 7.37 | 5.37 | 5.93 | 5.45 | 6.3 | 5.67 | 6.77 |
| Baichuan2-13B-Chat | 54.6 | 4.47 | 5.53 | 7.4 | 6.9 | 4.63 | 4.8 | 4.33 | 6.23 | 4.6 | 5.7 |
| Gemini-Pro | 54.5 | 4.87 | 5.43 | 7.07 | 6.43 | 5.10 | 4.5 | 4.65 | 6.33 | 4.42 | 5.7 |
| Qwen-Plus | 53.0 | 4.4 | 5.1 | 6.53 | 6.53 | 5.0 | 4.77 | 4.87 | 5.17 | 5.13 | 5.5 |
| Qwen-Turbo | 52.23 | 4.1 | 6.07 | 6.63 | 6.43 | 4.43 | 4.53 | 4.97 | 5.27 | 4.37 | 5.43 |
| Nanbeige-16B | 51.93 | 4.37 | 5.3 | 6.5 | 6.3 | 3.97 | 4.7 | 4.07 | 5.9 | 4.73 | 6.1 |
| GPT-3.5-Turbo | 51.9 | 4.97 | 5.37 | 6.4 | 6.47 | 4.43 | 4.67 | 5.43 | 4.2 | 4.37 | 5.6 |
| MiniMax-Abab5 | 51.83 | 3.87 | 5.63 | 6.87 | 6.97 | 4.33 | 4.4 | 2.93 | 6.13 | 4.27 | 6.43 |
| Mixtral-8x7B-Instruct | 48.4 | 4.27 | 5.47 | 6.47 | 6.4 | 3.13 | 4.5 | 5.07 | 3.57 | 4.37 | 5.17 |
| ChatGLM2-6B | 39.63 | 2.33 | 3.77 | 5.97 | 6.13 | 2.83 | 3.83 | 2.6 | 3.8 | 4.0 | 4.37 |
| Ziya-v1.1-13B | 37.63 | 2.77 | 3.97 | 5.17 | 5.33 | 2.8 | 3.77 | 2.53 | 3.7 | 3.03 | 4.57 |
| InternLM-Chat-7B | 36.27 | 2.63 | 3.67 | 4.87 | 5.57 | 3.17 | 3.33 | 2.33 | 4.03 | 3.13 | 3.53 |
| Linly-Chinese-LLaMA-2-13B-HF | 34.7 | 2.2 | 3.77 | 4.5 | 5.0 | 2.43 | 3.33 | 2.53 | 3.9 | 2.5 | 4.53 |
| BELLE-LLaMA2-13B-Chat-0.4M | 33.97 | 2.57 | 3.07 | 4.93 | 4.73 | 2.83 | 3.8 | 2.43 | 3.33 | 2.4 | 3.87 |
| LLaMA-2-7B-Chat-HF | 23.63 | 1.53 | 3.43 | 3.0 | 3.73 | 1.73 | 2.43 | 1.97 | 2.17 | 0.8 | 2.83 |

*注：学科分数采用10分制*

当前排名模型的性能表现在时间的分布如图所示：

<div align="center">
<img src=".\pic\trend_of_models.png" alt="模型性能发展趋势" style="zoom:80%;" />
</div>


更多的实验细节与实验分析请参考我们的[论文](https://arxiv.org/abs/2508.05452)。



## 📞 联系我们

本项目已经向公众开放，欢迎参与我们的评测。

机构评测需要进行认证，注册完账户以后，请联系管理员认证并申请评测权限。

如无特殊情况，在评测完成之后，相关结果都会添加在排行榜上。

- **网站**：[http://llmeval.com/](http://llmeval.com/)
- **邮箱**：mingzhang23@m.fudan.edu.cn
- **微信**：zanyingluan

---

<div align="center">

**LLMEval-3** | 构建LLM评测的未来

</div>
