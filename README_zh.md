<div align="center">
<h2>LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models</h2>

[![Paper](https://img.shields.io/badge/Paper-Arxiv-blue.svg?style=for-the-badge)](https://arxiv.org/abs/2508.05452)
[![Website](https://img.shields.io/badge/Website-LLMEval-green.svg?style=for-the-badge)](http://llmeval.com/)

</div>

<div align="center">
<img src=".\pic\llmeval-logo.png" alt="llmeval-logo" style="zoom:50%;" />
</div>

> **注意：** 英文版 README 请参阅 [README.md](README.md)。

## 📚 项目概述

LLMEval-3 是一个针对大语言模型（LLMs）进行**鲁棒性和公平性评测**的综合性大规模纵向基准测试平台。随着LLM技术的快速发展，LLMEval-3提供了一个动态评测框架，持续追踪2023年至今的模型能力演进，确保对模型性能进行一致且可靠的评估。

我们的基准测试覆盖教育部划定的**13个学科门类**，包括哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事学、管理学、艺术学，涵盖**50余个二级学科**，约**20万道题目**（目标扩展至100万）。

<div align="center">
<img src=".\pic\subjects.PNG" alt="学科覆盖范围" style="zoom:80%;" />
</div>

## 🎯 核心特色

### 🛡️ 鲁棒性评测设计
- **动态抽样机制**：每次评测随机抽取1000题，防止过拟合
- **多层防作弊保护**：全方位防范不公平行为
- **在线串行测试**：题目依次发送，防止批量爬取

### ⚖️ 公平性评估框架  
- **相对评分体系**：以当前最优模型为动态基准
- **绝对性能指标**：统一的0-100分制评分
- **时序一致性**：确保不同时期模型的公平比较

### 📈 纵向分析研究
- **持续追踪**：2023年至今的模型性能演进记录
- **趋势分析**：LLM能力发展的全面洞察
- **跨模型对比**：不同模型系列的标准化评估

## 🏆 当前排行榜（截至2025年8月）

### 顶尖表现模型

| 模型名称 | 发布机构 | 访问方式 | 评测日期 | 相对分数 (SOTA=100%) | 绝对分数 |
|----------|----------|----------|----------|---------------------|----------|
| **Doubao-1.5-Thinking-Pro** | 字节跳动 | API | 2025.7.21 | **100.00** | 93.67 |
| **DeepSeek-R1** | DeepSeek | API | 2025.7.21 | **97.40** | 91.23 |
| **Gemini-2.5-Pro-Preview** | Google | API | 2025.7.21 | **97.22** | 91.07 |
| **Gemini-2.5-Pro-Preview-Thinking** | Google | API | 2025.7.21 | **97.15** | 91.00 |
| **DeepSeek-V3** | DeepSeek | API | 2025.7.21 | **96.48** | 90.37 |

> 查看包含47+模型的[完整排行榜](#complete-results)。

## 🔬 评测方法

### 评测流程

1. **题目生成**：来源于权威学术资源
   - 大学本科课后作业和考试
   - 研究生入学考试
   - 专业资格认证材料

2. **防污染措施**：
   - 非公开数据源（PDF/Word文档）
   - OCR处理和数据清洗
   - 标准化格式协议

3. **评估协议**：
   - **生成式问答格式**：所有题目转换为开放式回答
   - **自动评分**：使用GPT-4 Turbo作为评测模型
   - **4分制评分**：基于正确性和解释质量的0-3分评分

### 📋 评测提示词

```text
Please evaluate the following response from the LLM regarding a discipline-specific question based on the following criteria. You must score it on a scale of 0, 1, 2 or 3 stars:

Overall Rating:
0 stars indicate wrong answer with a wrong explanation
1 star indicates wrong answer but a partially reasonable explanation
2 stars indicate a correct answer with a partially reasonable explanation
3 stars indicate a correct answer with a reasonable explanation

User: {question}

LLM:{answer_from_llm}

The correct answer to user's question is: {correct_answer}

You must provide your feedback in the following format:
{"Overall Rating":numbers of its stars(int)}
```

### 评分方法

为了规避由随机抽样1000题引入的系统偏差，LLMEval-3使用**相对分数**和**绝对分数**两个指标。

**相对分数计算：**
考虑到大语言模型技术的快速发展，我们引入相对分数来衡量模型与当前最佳性能的差距。我们选择榜单中表现最优的模型作为SOTA基准，目前为Doubao-1.5-Thinking-Pro：

$$R_{\text{SOTA}}^{\text{model}}=\frac{S_{model}}{S_\text{sotamodel}} \times 100 $$



**绝对分数计算：**
绝对分数表示模型在N=1000道题目上的原始表现，计算方式为将每题得分（0-3分）归一化到0-100区间：

$$S_{model}=\sum_{i=1}^N{\frac{s_i}{s_{max}} \times 100} \quad (1)$$

其中 $s_i$ 为第i题得分，$s_{max}$ = 3。

**评分说明：** $S_{\text{model}}$为绝对分数（0-100量表），$R_{\text{SOTA}}^{\text{model}}$为相对分数（以SOTA模型为100%基准），各学科分数采用10分制量表。

## 📊 研究发现

### 🚀 模型性能演进

<div align="center">
<img src=".\pic\trend_of_model_series.png" alt="模型性能发展趋势" style="zoom:80%;" />
</div>

我们的纵向分析揭示了所有模型系列的显著性能提升：

- **快速发展**：性能从GPT-3.5-turbo的51.9分跃升至Doubao-1.5-Thinking-Pro的93.67分
- **中文模型优异**：国产模型在中文学术领域展现出竞争力
- **学科差异**：STEM领域相比人文社科显示更高分数

### 🔍 评测可靠性验证

<div align="center">
<img src=".\pic\judge_model_performance_comparison.png" alt="评测模型一致性" style="zoom:80%;" />
</div>

我们的一致性分析表明：
- **GPT-4o卓越表现**：与人工评估者的Cohen's kappa一致性最高
- **自动评测可靠性**：自动评分与人工评分强相关
- **跨模型稳定性**：不同评测模型间的一致评估模式

### 📈 关键洞察

通过对2023年至2025年期间大语言模型的持续追踪评测，LLMEval-3揭示了模型发展的显著规律。最引人注目的发现是技术进步的加速趋势，模型性能呈现指数级改进模式，从根本上改变了AI能力的发展轨迹。性能指标从GPT-3.5-turbo的51.9分基线跃升至当前最先进的Doubao-1.5-Thinking-Pro的93.67分，在短短两年内实现了绝对意义上80%的性能提升。

我们的分析还发现了多语言能力的显著进步，特别是在中文语言理解和学术知识处理方面。国产中文模型展现出卓越的竞争实力，多个模型在特定学科评测中达到或超越国际基准水平。这一趋势表明本土化AI发展的成熟，以及区域特色语言模型能力的崛起。

评测方法本身也表现出高度的鲁棒性，我们的相对评分系统有效缓解了随机抽样通常引入的系统性偏差。跨时序一致性分析显示，保持较高相对分数的模型在多轮评测中展现出更稳定的性能模式，验证了我们动态基准方法的可靠性。此外，学科专业分析揭示了不同学术领域的独特性能模式，STEM领域相比人文社科通常显示更高的可达成分数，暗示了不同知识领域的复杂度阈值差异。

## 🎓 学术贡献

### 🏅 研究影响

LLMEval-3在大语言模型评测领域做出了开创性贡献，建立了首个系统性追踪多年跨度和多学科领域模型演进的综合纵向基准。我们的工作填补了现有评测方法的关键空白，通过提供持续、标准化的评估协议，实现了对不同时期开发模型的公平比较。这种纵向研究方法产生了前所未有的AI发展轨迹洞察，揭示了通过静态基准评测无法观察到的加速模式和能力涌现趋势。

我们引入的鲁棒相对评测框架构成了方法论创新，解决了跨时序模型比较中的根本公平性问题。通过实施针对当前最先进模型而非固定历史基线的动态基准评测，我们创建了一个随领域发展保持相关性和区分度的评测系统。这种方法在记录近期AI快速发展方面特别有价值，传统固定基准在快速发展的环境中往往很快变得饱和或过时。

我们对自动评估方法的全面验证为大规模AI评测系统的可靠性提供了关键实证证据。通过与人类专家判断的广泛比较，我们证明了精心设计的自动评测可以与人工评估达到高度一致性，同时实现综合模型比较所需的规模。这项验证工作对更广泛的AI评测社区具有重要意义，为未来大规模基准开发提供了方法论基础。

### 🔮 未来方向

- **规模扩展**：从20万题扩展至100万题
- **多模态集成**：文本-图像和数学公式评测
- **全球本地化**：多语言基准版本
- **实时平台**：在线评测和监控系统

## 🚀 快速开始

### 在线评测
访问我们的评测平台：**[http://llmeval.com/](http://llmeval.com/)**

### 机构接入
1. 在我们的平台注册账户
2. 联系管理员进行机构认证
3. 提交模型评测请求
4. 获取详细性能报告

### 评测流程
- **公平抽样**：每个模型接收独特的题目集
- **标准化协议**：一致的评测程序
- **全面报告**：详细的性能分析
- **排行榜集成**：自动排名更新

## 📋 完整结果 {#complete-results}

<details>
<summary>点击查看完整排行榜（47个模型）</summary>

| 模型名称 | 发布机构 | 访问方式 | 评测日期 | 相对分数 | 绝对分数 |
|----------|----------|----------|----------|----------|----------|
| Doubao-1.5-Thinking-Pro | 字节跳动 | API | 2025.7.21 | 100.00 | 93.67 |
| DeepSeek-R1 | DeepSeek | API | 2025.7.21 | 97.40 | 91.23 |
| Gemini-2.5-Pro-Preview | Google | API | 2025.7.21 | 97.22 | 91.07 |
| Gemini-2.5-Pro-Preview-Thinking | Google | API | 2025.7.21 | 97.15 | 91.00 |
| DeepSeek-V3 | DeepSeek | API | 2025.7.21 | 96.48 | 90.37 |
| Qwen3-235B | Alibaba Cloud | API | 2025.7.21 | 96.44 | 90.33 |
| Doubao-1.5-Pro-256K | 字节跳动 | API | 2025.7.21 | 95.69 | 89.63 |
| QwQ-32B | Alibaba Cloud | API | 2025.8.1 | 94.52 | 88.54 |
| O1-2024-12-17 | OpenAI | API | 2025.7.21 | 93.35 | 87.43 |
| Gemini-2.5-Flash-Thinking | Google | API | 2025.8.1 | 92.74 | 86.87 |
| Qwen3-32B | Alibaba Cloud | API | 2025.7.21 | 92.21 | 86.37 |
| Claude-Sonnet-4-Thinking | Anthropic | API | 2025.7.21 | 91.03 | 85.27 |
| Claude-Sonnet-4 | Anthropic | API | 2025.7.21 | 91.00 | 85.23 |
| GPT-4o-Search-Preview | OpenAI | API | 2025.7.21 | 89.40 | 83.73 |
| GLM-4-32B | Tsinghua&Zhipu.AI | API | 2025.8.1 | 88.43 | 82.83 |
| GPT-4o-2024-11-20 | OpenAI | API | 2025.7.21 | 88.08 | 82.50 |
| Gemini-1.5-Pro | Google | API | 2025.8.1 | 85.92 | 80.47 |
| Qwen2.5-32B-Instruct | Alibaba Cloud | API | 2025.8.1 | 85.07 | 79.68 |
| O3-Mini | OpenAI | API | 2025.7.21 | 84.13 | 78.80 |
| Qwen-Turbo-1101 | Alibaba Cloud | API | 2025.8.1 | 83.71 | 78.41 |
| Claude-3.5-Sonnet | Anthropic | API | 2025.8.1 | 83.38 | 78.10 |
| O1-Mini-2024-09-12 | OpenAI | API | 2025.8.1 | 78.93 | 73.93 |
| Claude-3-Haiku | Anthropic | API | 2025.8.1 | 62.95 | 58.97 |
| LLaMA-3.2-90B-Vision-Instruct | Meta | API | 2025.8.1 | 61.74 | 57.83 |
| LLaMA-3.3-70B | Meta | API | 2025.8.1 | 60.85 | 57.00 |
| Phi-3-Medium-128K-Instruct | Microsoft | API | 2025.8.1 | 36.94 | 34.60 |
| GPT-4 Turbo(gpt-4-1106-preview) | OpenAI | API | 2023.11.18 | 78.56 | 73.6 |
| GPT-4-0125-Preview | OpenAI | API | 2024.1.28 | 76.44 | 71.6 |
| 百度文心4.0 | 百度 | API | 2023.11.1 | 75.09 | 70.33 |
| Yi-34B-Chat | 零一万物 | API | 2023.12.1 | 70.17 | 65.70 |
| 百度文心3.5 | 百度 | API | 2023.11.1 | 69.14 | 64.73 |
| ChatGLM-Pro | 清华&智谱AI | API | 2023.11.1 | 69.14 | 64.73 |
| GPT-4-0613 | OpenAI | API | 2023.9.29 | 66.17 | 61.97 |
| 讯飞星火v3.0 | 科大讯飞 | API | 2023.11.7 | 65.64 | 61.47 |
| Nanbeige-Plus | 南北阁实验室 | API | 2023.12.1 | 65.14 | 61.00 |
| Baichuan2-13B-Chat | 百川智能 | 权重 | 2023.9.29 | 58.31 | 54.6 |
| Gemini-Pro | Google | API | 2024.1.10 | 58.20 | 54.5 |
| Qwen-Plus | 阿里云 | API | 2023.11.1 | 56.60 | 53.0 |
| Qwen-Turbo | 阿里云 | API | 2023.11.1 | 55.78 | 52.23 |
| Nanbeige-16B | 南北阁实验室 | API | 2023.10.23 | 55.46 | 51.93 |
| GPT-3.5-Turbo | OpenAI | API | 2023.9.29 | 55.42 | 51.9 |
| MiniMax-Abab5 | MiniMax | 权重 | 2023.11.1 | 55.33 | 51.83 |
| Mixtral-8x7B-Instruct | Mistral AI | 权重 | 2024.1.10 | 51.69 | 48.4 |
| ChatGLM2-6B | 清华&智谱AI | 权重 | 2023.9.29 | 42.32 | 39.63 |
| Ziya-v1.1-13B | IDEA研究院 | 权重 | 2023.9.29 | 40.18 | 37.63 |
| InternLM-Chat-7B | 上海AI实验室&商汤 | 权重 | 2023.9.29 | 38.73 | 36.27 |
| Linly-Chinese-LLaMA-2-13B-HF | 大数据系统计算技术国家工程实验室 | 权重 | 2023.10.3 | 37.06 | 34.7 |
| BELLE-LLaMA2-13B-Chat-0.4M | 链家科技 | 权重 | 2023.10.1 | 36.28 | 33.97 |
| LLaMA-2-7B-Chat-HF | Meta | 权重 | 2023.9.29 | 25.24 | 23.63 |

</details>

## 📊 学科专业表现 {#discipline-performance}

<details>
<summary>查看各学科领域性能分解</summary>

| 模型名称 | 总分 | 工学 | 经济学 | 教育学 | 法学 | 文学 | 管理学 | 理学 | 历史学 | 医学 | 军事学 |
|----------|------|------|--------|--------|------|------|--------|------|--------|------|--------|
| Doubao-1.5-Thinking-Pro | 93.67 | 9.47 | 9.67 | 9.43 | 9.77 | 8.93 | 9.53 | 9.23 | 9.70 | 8.97 | 8.97 |
| DeepSeek-R1 | 91.23 | 9.47 | 9.43 | 9.27 | 9.37 | 8.83 | 9.37 | 9.03 | 9.53 | 8.50 | 8.43 |
| Gemini-2.5-Pro-Preview | 91.07 | 9.20 | 9.47 | 9.20 | 9.30 | 8.43 | 9.63 | 9.07 | 9.40 | 8.50 | 8.87 |
| Gemini-2.5-Pro-Preview-Thinking | 91.00 | 9.13 | 9.50 | 9.37 | 9.47 | 8.40 | 9.63 | 9.20 | 9.27 | 8.30 | 8.73 |
| DeepSeek-V3 | 90.37 | 9.30 | 9.57 | 8.93 | 9.23 | 8.60 | 9.13 | 8.97 | 9.47 | 8.83 | 8.33 |
| Qwen3-235B | 90.33 | 9.23 | 9.43 | 9.03 | 9.50 | 8.23 | 9.43 | 8.97 | 9.17 | 8.73 | 8.60 |
| Doubao-1.5-Pro-256K | 89.63 | 8.83 | 9.03 | 9.13 | 9.43 | 8.57 | 9.27 | 8.83 | 9.10 | 8.60 | 8.83 |
| QwQ-32B | 88.54 | 8.30 | 9.46 | 9.23 | 9.33 | 7.83 | 9.46 | 8.65 | 9.27 | 8.57 | 8.43 |
| O1-2024-12-17 | 87.43 | 8.90 | 9.30 | 8.67 | 8.77 | 7.73 | 9.27 | 8.90 | 8.97 | 8.17 | 8.77 |
| Gemini-2.5-Flash-Thinking | 86.87 | 8.67 | 9.27 | 8.70 | 9.00 | 7.80 | 8.93 | 8.90 | 9.00 | 8.03 | 8.57 |
| Qwen3-32B | 86.37 | 8.43 | 9.10 | 8.57 | 9.10 | 7.77 | 9.47 | 8.67 | 9.30 | 7.70 | 8.27 |
| Claude-Sonnet-4-Thinking | 85.27 | 8.57 | 9.00 | 8.63 | 8.73 | 7.57 | 9.10 | 8.93 | 8.70 | 7.97 | 8.07 |
| Claude-Sonnet-4 | 85.23 | 8.57 | 8.80 | 8.50 | 8.70 | 7.80 | 9.03 | 8.80 | 8.80 | 8.17 | 8.07 |
| GPT-4o-Search-Preview | 83.73 | 8.27 | 8.77 | 8.43 | 8.67 | 7.77 | 8.80 | 8.20 | 8.73 | 8.27 | 7.83 |
| GLM-4-32B | 82.83 | 7.77 | 8.97 | 8.33 | 8.33 | 7.03 | 9.13 | 8.27 | 8.77 | 8.23 | 8.00 |
| GPT-4o-2024-11-20 | 82.50 | 7.90 | 8.67 | 8.30 | 8.33 | 7.17 | 8.97 | 8.57 | 8.67 | 7.63 | 8.30 |
| Gemini-1.5-Pro | 80.47 | 8.13 | 8.45 | 8.30 | 8.37 | 7.04 | 8.17 | 8.43 | 8.50 | 7.48 | 7.60 |
| Qwen2.5-32B-Instruct | 79.68 | 7.70 | 8.57 | 8.33 | 8.33 | 6.70 | 8.50 | 8.17 | 7.70 | 7.60 | 8.08 |
| O3-Mini | 78.80 | 7.97 | 8.60 | 8.30 | 8.20 | 6.73 | 8.57 | 8.53 | 7.17 | 7.03 | 7.70 |
| Qwen-Turbo-1101 | 78.41 | 7.97 | 8.37 | 8.03 | 8.23 | 6.40 | 8.50 | 8.10 | 7.50 | 7.27 | 8.05 |
| Claude-3.5-Sonnet | 78.10 | 7.97 | 8.53 | 8.27 | 7.93 | 7.03 | 8.50 | 8.00 | 7.57 | 6.70 | 7.60 |
| O1-Mini-2024-09-12 | 73.93 | 7.27 | 8.43 | 7.90 | 7.53 | 6.27 | 8.27 | 8.17 | 6.43 | 6.63 | 7.03 |
| Claude-3-Haiku | 58.97 | 5.80 | 6.60 | 6.97 | 6.63 | 4.83 | 5.93 | 6.33 | 4.80 | 5.23 | 5.83 |
| LLaMA-3.2-90B-Vision-Instruct | 57.83 | 5.63 | 6.33 | 6.20 | 5.80 | 4.73 | 6.10 | 6.57 | 5.03 | 5.27 | 6.17 |
| LLaMA-3.3-70B | 57.00 | 5.80 | 6.90 | 5.63 | 5.70 | 5.47 | 5.70 | 6.30 | 4.70 | 4.87 | 5.93 |
| Phi-3-Medium-128K-Instruct | 34.60 | 2.27 | 4.17 | 3.70 | 4.23 | 2.87 | 4.50 | 3.57 | 3.20 | 2.27 | 3.83 |
| GPT-4 Turbo(gpt-4-1106-preview) | 73.6 | 6.97 | 8.17 | 8.33 | 7.8 | 6.0 | 7.57 | 8.13 | 7.0 | 6.43 | 7.2 |
| GPT-4-0125-Preview | 71.6 | 6.9 | 7.4 | 8.03 | 7.3 | 6.0 | 7.47 | 7.63 | 6.87 | 6.33 | 7.67 |
| 百度文心4.0 | 70.33 | 7.27 | 7.23 | 7.67 | 7.43 | 5.63 | 6.47 | 6.8 | 7.63 | 7.8 | 6.4 |
| Yi-34B-Chat | 65.70 | 5.77 | 6.63 | 7.37 | 7.53 | 5.47 | 5.77 | 5.47 | 7.47 | 6.3 | 7.93 |
| 百度文心3.5 | 64.73 | 6.2 | 6.7 | 7.8 | 6.83 | 5.2 | 5.5 | 6.0 | 7.23 | 6.57 | 6.7 |
| ChatGLM-Pro | 64.73 | 5.9 | 7.07 | 7.03 | 7.9 | 5.43 | 6.33 | 5.0 | 6.67 | 5.97 | 7.43 |
| GPT-4-0613 | 61.97 | 6.5 | 6.73 | 6.6 | 6.73 | 5.43 | 6.1 | 6.47 | 5.3 | 5.2 | 6.9 |
| 讯飞星火v3.0 | 61.47 | 5.77 | 6.5 | 7.27 | 7.3 | 5.7 | 5.9 | 5.03 | 6.5 | 5.23 | 6.27 |
| Nanbeige-Plus | 61.00 | 5.78 | 5.57 | 6.77 | 7.37 | 5.37 | 5.93 | 5.45 | 6.3 | 5.67 | 6.77 |
| Baichuan2-13B-Chat | 54.6 | 4.47 | 5.53 | 7.4 | 6.9 | 4.63 | 4.8 | 4.33 | 6.23 | 4.6 | 5.7 |
| Gemini-Pro | 54.5 | 4.87 | 5.43 | 7.07 | 6.43 | 5.10 | 4.5 | 4.65 | 6.33 | 4.42 | 5.7 |
| Qwen-Plus | 53.0 | 4.4 | 5.1 | 6.53 | 6.53 | 5.0 | 4.77 | 4.87 | 5.17 | 5.13 | 5.5 |
| Qwen-Turbo | 52.23 | 4.1 | 6.07 | 6.63 | 6.43 | 4.43 | 4.53 | 4.97 | 5.27 | 4.37 | 5.43 |
| Nanbeige-16B | 51.93 | 4.37 | 5.3 | 6.5 | 6.3 | 3.97 | 4.7 | 4.07 | 5.9 | 4.73 | 6.1 |
| GPT-3.5-Turbo | 51.9 | 4.97 | 5.37 | 6.4 | 6.47 | 4.43 | 4.67 | 5.43 | 4.2 | 4.37 | 5.6 |
| MiniMax-Abab5 | 51.83 | 3.87 | 5.63 | 6.87 | 6.97 | 4.33 | 4.4 | 2.93 | 6.13 | 4.27 | 6.43 |
| Mixtral-8x7B-Instruct | 48.4 | 4.27 | 5.47 | 6.47 | 6.4 | 3.13 | 4.5 | 5.07 | 3.57 | 4.37 | 5.17 |
| ChatGLM2-6B | 39.63 | 2.33 | 3.77 | 5.97 | 6.13 | 2.83 | 3.83 | 2.6 | 3.8 | 4.0 | 4.37 |
| Ziya-v1.1-13B | 37.63 | 2.77 | 3.97 | 5.17 | 5.33 | 2.8 | 3.77 | 2.53 | 3.7 | 3.03 | 4.57 |
| InternLM-Chat-7B | 36.27 | 2.63 | 3.67 | 4.87 | 5.57 | 3.17 | 3.33 | 2.33 | 4.03 | 3.13 | 3.53 |
| Linly-Chinese-LLaMA-2-13B-HF | 34.7 | 2.2 | 3.77 | 4.5 | 5.0 | 2.43 | 3.33 | 2.53 | 3.9 | 2.5 | 4.53 |
| BELLE-LLaMA2-13B-Chat-0.4M | 33.97 | 2.57 | 3.07 | 4.93 | 4.73 | 2.83 | 3.8 | 2.43 | 3.33 | 2.4 | 3.87 |
| LLaMA-2-7B-Chat-HF | 23.63 | 1.53 | 3.43 | 3.0 | 3.73 | 1.73 | 2.43 | 1.97 | 2.17 | 0.8 | 2.83 |

*注：学科分数采用10分制*

</details>

## 🤝 贡献参与

我们欢迎研究社区的贡献！请随时：

- 提交问题报告或功能请求
- 提出新的评测指标或方法
- 贡献额外的题目集或领域
- 建议改进我们的评测框架

## 📞 联系我们

如有问题、建议或合作机会：

- **网站**：[http://llmeval.com/](http://llmeval.com/)
- **邮箱**：mingzhang23@m.fudan.edu.cn
- **微信**：zanyingluan

## 📄 引用

如果您在研究中使用了LLMEval-3，请引用我们的论文：

```bibtex
@article{zhang2024llmeval3,
  title={LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models},
  author={Zhang, Ming and Shen, Yujiong and others},
  journal={arXiv preprint arXiv:2508.05452},
  year={2024}
}
```

## 📜 许可证

本项目采用我们仓库中指定的许可证条款。详情请参阅LICENSE文件。

---

<div align="center">

**LLMEval-3** | 构建LLM评测的未来

</div>
