<div align="center">
<h2>LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models</h2>

[![Paper](https://img.shields.io/badge/Paper-Arxiv-blue.svg?style=for-the-badge)](https://arxiv.org/abs/2508.05452)
[![Website](https://img.shields.io/badge/Website-LLMEval-green.svg?style=for-the-badge)](http://llmeval.com/)

</div>

<div align="center">
<img src=".\pic\llmeval-logo.png" alt="llmeval-logo" style="zoom:50%;" />
</div>

> **注意：** 英文版 README 请参阅 [README.md](README.md)。

## 📚 项目概述

LLMEval-3 是一个针对大语言模型（LLMs）进行**鲁棒性和公平性评测**的综合性大规模纵向基准测试平台。随着LLM技术的快速发展，LLMEval-3提供了一个动态评测框架，持续追踪2023年至今的模型能力演进，确保对模型性能进行一致且可靠的评估。

我们的基准测试覆盖教育部划定的**13个学科门类**，包括哲学、经济学、法学、教育学、文学、历史学、理学、工学、农学、医学、军事学、管理学、艺术学，涵盖**50余个二级学科**，约**20万道题目**（目标扩展至100万）。

<div align="center">
<img src=".\pic\subjects.PNG" alt="学科覆盖范围" style="zoom:80%;" />
</div>

## 🎯 核心特色

### 🛡️ 鲁棒性评测设计
- **动态抽样机制**：每次评测随机抽取1000题，防止过拟合
- **多层防作弊保护**：全方位防范不公平行为
- **在线串行测试**：题目依次发送，防止批量爬取

### ⚖️ 公平性评估框架  
- **相对评分体系**：以当前最优模型为动态基准
- **绝对性能指标**：统一的0-100分制评分
- **时序一致性**：确保不同时期模型的公平比较

### 📈 纵向分析研究
- **持续追踪**：2023年至今的模型性能演进记录
- **趋势分析**：LLM能力发展的全面洞察
- **跨模型对比**：不同模型系列的标准化评估

## 🏆 当前排行榜（截至2025年8月）

### 顶尖表现模型

| 模型名称 | 发布机构 | 访问方式 | 评测日期 | 相对分数 (SOTA=100%) | 绝对分数 |
|----------|----------|----------|----------|---------------------|----------|
| **Doubao-1.5-Thinking-Pro** | 字节跳动 | API | 2025.7.21 | **100.00** | 93.67 |
| **DeepSeek-R1** | DeepSeek | API | 2025.7.21 | **97.40** | 91.23 |
| **Gemini-2.5-Pro-Preview** | Google | API | 2025.7.21 | **97.22** | 91.07 |
| **Gemini-2.5-Pro-Preview-Thinking** | Google | API | 2025.7.21 | **97.15** | 91.00 |
| **DeepSeek-V3** | DeepSeek | API | 2025.7.21 | **96.48** | 90.37 |

### 📋 完整结果（47个模型）

| 模型名称 | 发布机构 | 访问方式 | 评测日期 | 相对分数 | 绝对分数 |
|----------|----------|----------|----------|----------|----------|
| Doubao-1.5-Thinking-Pro | 字节跳动 | API | 2025.7.21 | 100.00 | 93.67 |
| DeepSeek-R1 | DeepSeek | API | 2025.7.21 | 97.40 | 91.23 |
| Gemini-2.5-Pro-Preview | Google | API | 2025.7.21 | 97.22 | 91.07 |
| Gemini-2.5-Pro-Preview-Thinking | Google | API | 2025.7.21 | 97.15 | 91.00 |
| DeepSeek-V3 | DeepSeek | API | 2025.7.21 | 96.48 | 90.37 |
| Qwen3-235B | Alibaba Cloud | API | 2025.7.21 | 96.44 | 90.33 |
| Doubao-1.5-Pro-256K | 字节跳动 | API | 2025.7.21 | 95.69 | 89.63 |
| QwQ-32B | Alibaba Cloud | API | 2025.8.1 | 94.52 | 88.54 |
| O1-2024-12-17 | OpenAI | API | 2025.7.21 | 93.35 | 87.43 |
| Gemini-2.5-Flash-Thinking | Google | API | 2025.8.1 | 92.74 | 86.87 |
| Qwen3-32B | Alibaba Cloud | API | 2025.7.21 | 92.21 | 86.37 |
| Claude-Sonnet-4-Thinking | Anthropic | API | 2025.7.21 | 91.03 | 85.27 |
| Claude-Sonnet-4 | Anthropic | API | 2025.7.21 | 91.00 | 85.23 |
| GPT-4o-Search-Preview | OpenAI | API | 2025.7.21 | 89.40 | 83.73 |
| GLM-4-32B | Tsinghua&Zhipu.AI | API | 2025.8.1 | 88.43 | 82.83 |
| GPT-4o-2024-11-20 | OpenAI | API | 2025.7.21 | 88.08 | 82.50 |
| Gemini-1.5-Pro | Google | API | 2025.8.1 | 85.92 | 80.47 |
| Qwen2.5-32B-Instruct | Alibaba Cloud | API | 2025.8.1 | 85.07 | 79.68 |
| O3-Mini | OpenAI | API | 2025.7.21 | 84.13 | 78.80 |
| Qwen-Turbo-1101 | Alibaba Cloud | API | 2025.8.1 | 83.71 | 78.41 |
| Claude-3.5-Sonnet | Anthropic | API | 2025.8.1 | 83.38 | 78.10 |
| O1-Mini-2024-09-12 | OpenAI | API | 2025.8.1 | 78.93 | 73.93 |
| Claude-3-Haiku | Anthropic | API | 2025.8.1 | 62.95 | 58.97 |
| LLaMA-3.2-90B-Vision-Instruct | Meta | API | 2025.8.1 | 61.74 | 57.83 |
| LLaMA-3.3-70B | Meta | API | 2025.8.1 | 60.85 | 57.00 |
| Phi-3-Medium-128K-Instruct | Microsoft | API | 2025.8.1 | 36.94 | 34.60 |
| GPT-4 Turbo(gpt-4-1106-preview) | OpenAI | API | 2023.11.18 | 78.56 | 73.6 |
| GPT-4-0125-Preview | OpenAI | API | 2024.1.28 | 76.44 | 71.6 |
| 百度文心4.0 | 百度 | API | 2023.11.1 | 75.09 | 70.33 |
| Yi-34B-Chat | 零一万物 | API | 2023.12.1 | 70.17 | 65.70 |
| 百度文心3.5 | 百度 | API | 2023.11.1 | 69.14 | 64.73 |
| ChatGLM-Pro | 清华&智谱AI | API | 2023.11.1 | 69.14 | 64.73 |
| GPT-4-0613 | OpenAI | API | 2023.9.29 | 66.17 | 61.97 |
| 讯飞星火v3.0 | 科大讯飞 | API | 2023.11.7 | 65.64 | 61.47 |
| Nanbeige-Plus | 南北阁实验室 | API | 2023.12.1 | 65.14 | 61.00 |
| Baichuan2-13B-Chat | 百川智能 | 权重 | 2023.9.29 | 58.31 | 54.6 |
| Gemini-Pro | Google | API | 2024.1.10 | 58.20 | 54.5 |
| Qwen-Plus | 阿里云 | API | 2023.11.1 | 56.60 | 53.0 |
| Qwen-Turbo | 阿里云 | API | 2023.11.1 | 55.78 | 52.23 |
| Nanbeige-16B | 南北阁实验室 | API | 2023.10.23 | 55.46 | 51.93 |
| GPT-3.5-Turbo | OpenAI | API | 2023.9.29 | 55.42 | 51.9 |
| MiniMax-Abab5 | MiniMax | 权重 | 2023.11.1 | 55.33 | 51.83 |
| Mixtral-8x7B-Instruct | Mistral AI | 权重 | 2024.1.10 | 51.69 | 48.4 |
| ChatGLM2-6B | 清华&智谱AI | 权重 | 2023.9.29 | 42.32 | 39.63 |
| Ziya-v1.1-13B | IDEA研究院 | 权重 | 2023.9.29 | 40.18 | 37.63 |
| InternLM-Chat-7B | 上海AI实验室&商汤 | 权重 | 2023.9.29 | 38.73 | 36.27 |
| Linly-Chinese-LLaMA-2-13B-HF | 大数据系统计算技术国家工程实验室 | 权重 | 2023.10.3 | 37.06 | 34.7 |
| BELLE-LLaMA2-13B-Chat-0.4M | 链家科技 | 权重 | 2023.10.1 | 36.28 | 33.97 |
| LLaMA-2-7B-Chat-HF | Meta | 权重 | 2023.9.29 | 25.24 | 23.63 |

### 📊 学科专业表现

| 模型名称 | 总分 | 工学 | 经济学 | 教育学 | 法学 | 文学 | 管理学 | 理学 | 历史学 | 医学 | 军事学 |
|----------|------|------|--------|--------|------|------|--------|------|--------|------|--------|
| Doubao-1.5-Thinking-Pro | 93.67 | 9.47 | 9.67 | 9.43 | 9.77 | 8.93 | 9.53 | 9.23 | 9.70 | 8.97 | 8.97 |
| DeepSeek-R1 | 91.23 | 9.47 | 9.43 | 9.27 | 9.37 | 8.83 | 9.37 | 9.03 | 9.53 | 8.50 | 8.43 |
| Gemini-2.5-Pro-Preview | 91.07 | 9.20 | 9.47 | 9.20 | 9.30 | 8.43 | 9.63 | 9.07 | 9.40 | 8.50 | 8.87 |
| Gemini-2.5-Pro-Preview-Thinking | 91.00 | 9.13 | 9.50 | 9.37 | 9.47 | 8.40 | 9.63 | 9.20 | 9.27 | 8.30 | 8.73 |
| DeepSeek-V3 | 90.37 | 9.30 | 9.57 | 8.93 | 9.23 | 8.60 | 9.13 | 8.97 | 9.47 | 8.83 | 8.33 |
| Qwen3-235B | 90.33 | 9.23 | 9.43 | 9.03 | 9.50 | 8.23 | 9.43 | 8.97 | 9.17 | 8.73 | 8.60 |
| Doubao-1.5-Pro-256K | 89.63 | 8.83 | 9.03 | 9.13 | 9.43 | 8.57 | 9.27 | 8.83 | 9.10 | 8.60 | 8.83 |
| QwQ-32B | 88.54 | 8.30 | 9.46 | 9.23 | 9.33 | 7.83 | 9.46 | 8.65 | 9.27 | 8.57 | 8.43 |
| O1-2024-12-17 | 87.43 | 8.90 | 9.30 | 8.67 | 8.77 | 7.73 | 9.27 | 8.90 | 8.97 | 8.17 | 8.77 |
| Gemini-2.5-Flash-Thinking | 86.87 | 8.67 | 9.27 | 8.70 | 9.00 | 7.80 | 8.93 | 8.90 | 9.00 | 8.03 | 8.57 |
| Qwen3-32B | 86.37 | 8.43 | 9.10 | 8.57 | 9.10 | 7.77 | 9.47 | 8.67 | 9.30 | 7.70 | 8.27 |
| Claude-Sonnet-4-Thinking | 85.27 | 8.57 | 9.00 | 8.63 | 8.73 | 7.57 | 9.10 | 8.93 | 8.70 | 7.97 | 8.07 |
| Claude-Sonnet-4 | 85.23 | 8.57 | 8.80 | 8.50 | 8.70 | 7.80 | 9.03 | 8.80 | 8.80 | 8.17 | 8.07 |
| GPT-4o-Search-Preview | 83.73 | 8.27 | 8.77 | 8.43 | 8.67 | 7.77 | 8.80 | 8.20 | 8.73 | 8.27 | 7.83 |
| GLM-4-32B | 82.83 | 7.77 | 8.97 | 8.33 | 8.33 | 7.03 | 9.13 | 8.27 | 8.77 | 8.23 | 8.00 |
| GPT-4o-2024-11-20 | 82.50 | 7.90 | 8.67 | 8.30 | 8.33 | 7.17 | 8.97 | 8.57 | 8.67 | 7.63 | 8.30 |
| Gemini-1.5-Pro | 80.47 | 8.13 | 8.45 | 8.30 | 8.37 | 7.04 | 8.17 | 8.43 | 8.50 | 7.48 | 7.60 |
| Qwen2.5-32B-Instruct | 79.68 | 7.70 | 8.57 | 8.33 | 8.33 | 6.70 | 8.50 | 8.17 | 7.70 | 7.60 | 8.08 |
| O3-Mini | 78.80 | 7.97 | 8.60 | 8.30 | 8.20 | 6.73 | 8.57 | 8.53 | 7.17 | 7.03 | 7.70 |
| Qwen-Turbo-1101 | 78.41 | 7.97 | 8.37 | 8.03 | 8.23 | 6.40 | 8.50 | 8.10 | 7.50 | 7.27 | 8.05 |
| Claude-3.5-Sonnet | 78.10 | 7.97 | 8.53 | 8.27 | 7.93 | 7.03 | 8.50 | 8.00 | 7.57 | 6.70 | 7.60 |
| O1-Mini-2024-09-12 | 73.93 | 7.27 | 8.43 | 7.90 | 7.53 | 6.27 | 8.27 | 8.17 | 6.43 | 6.63 | 7.03 |
| Claude-3-Haiku | 58.97 | 5.80 | 6.60 | 6.97 | 6.63 | 4.83 | 5.93 | 6.33 | 4.80 | 5.23 | 5.83 |
| LLaMA-3.2-90B-Vision-Instruct | 57.83 | 5.63 | 6.33 | 6.20 | 5.80 | 4.73 | 6.10 | 6.57 | 5.03 | 5.27 | 6.17 |
| LLaMA-3.3-70B | 57.00 | 5.80 | 6.90 | 5.63 | 5.70 | 5.47 | 5.70 | 6.30 | 4.70 | 4.87 | 5.93 |
| Phi-3-Medium-128K-Instruct | 34.60 | 2.27 | 4.17 | 3.70 | 4.23 | 2.87 | 4.50 | 3.57 | 3.20 | 2.27 | 3.83 |
| GPT-4 Turbo(gpt-4-1106-preview) | 73.6 | 6.97 | 8.17 | 8.33 | 7.8 | 6.0 | 7.57 | 8.13 | 7.0 | 6.43 | 7.2 |
| GPT-4-0125-Preview | 71.6 | 6.9 | 7.4 | 8.03 | 7.3 | 6.0 | 7.47 | 7.63 | 6.87 | 6.33 | 7.67 |
| 百度文心4.0 | 70.33 | 7.27 | 7.23 | 7.67 | 7.43 | 5.63 | 6.47 | 6.8 | 7.63 | 7.8 | 6.4 |
| Yi-34B-Chat | 65.70 | 5.77 | 6.63 | 7.37 | 7.53 | 5.47 | 5.77 | 5.47 | 7.47 | 6.3 | 7.93 |
| 百度文心3.5 | 64.73 | 6.2 | 6.7 | 7.8 | 6.83 | 5.2 | 5.5 | 6.0 | 7.23 | 6.57 | 6.7 |
| ChatGLM-Pro | 64.73 | 5.9 | 7.07 | 7.03 | 7.9 | 5.43 | 6.33 | 5.0 | 6.67 | 5.97 | 7.43 |
| GPT-4-0613 | 61.97 | 6.5 | 6.73 | 6.6 | 6.73 | 5.43 | 6.1 | 6.47 | 5.3 | 5.2 | 6.9 |
| 讯飞星火v3.0 | 61.47 | 5.77 | 6.5 | 7.27 | 7.3 | 5.7 | 5.9 | 5.03 | 6.5 | 5.23 | 6.27 |
| Nanbeige-Plus | 61.00 | 5.78 | 5.57 | 6.77 | 7.37 | 5.37 | 5.93 | 5.45 | 6.3 | 5.67 | 6.77 |
| Baichuan2-13B-Chat | 54.6 | 4.47 | 5.53 | 7.4 | 6.9 | 4.63 | 4.8 | 4.33 | 6.23 | 4.6 | 5.7 |
| Gemini-Pro | 54.5 | 4.87 | 5.43 | 7.07 | 6.43 | 5.10 | 4.5 | 4.65 | 6.33 | 4.42 | 5.7 |
| Qwen-Plus | 53.0 | 4.4 | 5.1 | 6.53 | 6.53 | 5.0 | 4.77 | 4.87 | 5.17 | 5.13 | 5.5 |
| Qwen-Turbo | 52.23 | 4.1 | 6.07 | 6.63 | 6.43 | 4.43 | 4.53 | 4.97 | 5.27 | 4.37 | 5.43 |
| Nanbeige-16B | 51.93 | 4.37 | 5.3 | 6.5 | 6.3 | 3.97 | 4.7 | 4.07 | 5.9 | 4.73 | 6.1 |
| GPT-3.5-Turbo | 51.9 | 4.97 | 5.37 | 6.4 | 6.47 | 4.43 | 4.67 | 5.43 | 4.2 | 4.37 | 5.6 |
| MiniMax-Abab5 | 51.83 | 3.87 | 5.63 | 6.87 | 6.97 | 4.33 | 4.4 | 2.93 | 6.13 | 4.27 | 6.43 |
| Mixtral-8x7B-Instruct | 48.4 | 4.27 | 5.47 | 6.47 | 6.4 | 3.13 | 4.5 | 5.07 | 3.57 | 4.37 | 5.17 |
| ChatGLM2-6B | 39.63 | 2.33 | 3.77 | 5.97 | 6.13 | 2.83 | 3.83 | 2.6 | 3.8 | 4.0 | 4.37 |
| Ziya-v1.1-13B | 37.63 | 2.77 | 3.97 | 5.17 | 5.33 | 2.8 | 3.77 | 2.53 | 3.7 | 3.03 | 4.57 |
| InternLM-Chat-7B | 36.27 | 2.63 | 3.67 | 4.87 | 5.57 | 3.17 | 3.33 | 2.33 | 4.03 | 3.13 | 3.53 |
| Linly-Chinese-LLaMA-2-13B-HF | 34.7 | 2.2 | 3.77 | 4.5 | 5.0 | 2.43 | 3.33 | 2.53 | 3.9 | 2.5 | 4.53 |
| BELLE-LLaMA2-13B-Chat-0.4M | 33.97 | 2.57 | 3.07 | 4.93 | 4.73 | 2.83 | 3.8 | 2.43 | 3.33 | 2.4 | 3.87 |
| LLaMA-2-7B-Chat-HF | 23.63 | 1.53 | 3.43 | 3.0 | 3.73 | 1.73 | 2.43 | 1.97 | 2.17 | 0.8 | 2.83 |

*注：学科分数采用10分制*

## 🔬 评测方法

### 评测流程

1. **题目生成**：来源于权威学术资源
   - 大学本科课后作业和考试
   - 研究生入学考试
   - 专业资格认证材料

2. **防污染措施**：
   - 非公开数据源（PDF/Word文档）
   - OCR处理和数据清洗
   - 标准化格式协议

3. **评估协议**：
   - **生成式问答格式**：所有题目转换为开放式回答
   - **自动评分**：使用GPT-4 Turbo作为评测模型
   - **4分制评分**：基于正确性和解释质量的0-3分评分

### 📋 评测提示词

```text
Please evaluate the following response from the LLM regarding a discipline-specific question based on the following criteria. You must score it on a scale of 0, 1, 2 or 3 stars:

Overall Rating:
0 stars indicate wrong answer with a wrong explanation
1 star indicates wrong answer but a partially reasonable explanation
2 stars indicate a correct answer with a partially reasonable explanation
3 stars indicate a correct answer with a reasonable explanation

User: {question}

LLM:{answer_from_llm}

The correct answer to user's question is: {correct_answer}

You must provide your feedback in the following format:
{"Overall Rating":numbers of its stars(int)}
```

### 评分方法

为了规避由随机抽样1000题引入的系统偏差，LLMEval-3使用**相对分数**和**绝对分数**两个指标。

**相对分数计算：**
考虑到大语言模型技术的快速发展，我们引入相对分数来衡量模型与当前最佳性能的差距。我们选择榜单中表现最优的模型作为SOTA基准，目前为Doubao-1.5-Thinking-Pro：

$$R_{\text{SOTA}}^{\text{model}}=\frac{S_{model}}{S_\text{sotamodel}} \times 100 $$



**绝对分数计算：**
绝对分数表示模型在N=1000道题目上的原始表现，计算方式为将每题得分（0-3分）归一化到0-100区间：

$$S_{model}=\sum_{i=1}^N{\frac{s_i}{s_{max}} \times 100} \quad (1)$$

其中 $s_i$ 为第i题得分， $s_{max}=3$ 。

**评分说明：** $S_{\text{model}}$为绝对分数（0-100量表）， $R_{\text{SOTA}}^{\text{model}}$ 为相对分数（以SOTA模型为100%基准），各学科分数采用10分制量表。

## 📊 研究发现

### 🚀 模型性能演进

<div align="center">
<img src=".\pic\trend_of_model_series.png" alt="模型性能发展趋势" style="zoom:80%;" />
</div>

我们的纵向分析揭示了所有模型系列的显著性能提升，数据中呈现出几个关键的演进模式：

**指数级增长轨迹**：散点图展示了模型能力的显著加速发展，性能指标呈现指数级而非线性的改进曲线。从GPT-3.5-turbo的51.9分基线到Doubao-1.5-Thinking-Pro当前93.67分的峰值，代表了在短短两年内绝对意义上80%的性能提升。这种加速模式表明我们正在见证大语言模型发展的技术拐点。

**模型家族聚类与竞争**：可视化图表揭示了不同模型家族间的明显聚类模式，各大AI实验室间呈现出清晰的竞争动态。OpenAI的GPT系列、Google的Gemini家族、Anthropic的Claude模型，以及中国国产模型（包括豆包、DeepSeek和通义千问系列）各自展现出独特的发展轨迹。值得注意的是，中国模型生态系统已实现与国际基准的显著趋同，多个国产模型现已占据性能谱系的顶级位置。

**时序性能密度**：数据点显示在高性能范围内随时间增加的密度，表明该领域正在经历集体进步而非孤立突破。这表明基础算法创新和计算进步在研究社区中具有广泛可及性，导致多个模型家族的同步改进。

**性能饱和阈值**：散点图揭示了在特定分数范围内新兴的性能饱和效应，特别是在85-95分阈值附近，增量改进变得越来越困难。这种模式表明当前评测方法可能正在接近对最高性能模型的区分极限，突出了对更具挑战性基准任务的需求。

### 🔍 评测可靠性验证

<div align="center">
<img src=".\pic\judge_model_performance_comparison.png" alt="评测模型一致性" style="zoom:80%;" />
</div>

我们的综合一致性分析为自动评测方法提供了关键验证，揭示了AI评估系统可靠性和鲁棒性的几个重要发现：

**GPT-4o作为黄金标准评测模型**：箱线图分析表明GPT-4o与人类专家评估者达到了最高的Cohen's kappa一致性，其中位数kappa值显著超越其他评测模型。紧密的四分位距和最少的异常值分布表明其在不同题目类型和难度级别上具有一致且可靠的评分行为。这种卓越表现验证了我们选择GPT-4o作为LLMEval-3主要评测模型的决策，确保自动评分与人类专家判断密切一致。

**比较评测模型分析**：可视化图表揭示了不同评测模型间的显著性能差异，在其与人类评估者的一致性方面呈现出清晰的层次模式。虽然GPT-4o在一致性方面领先，其他模型显示出不同程度的可靠性，一些模型表现出更宽的方差范围和更频繁的异常值。这一分析强调了在自动评测系统中评测模型选择的关键重要性，并证明并非所有大语言模型都同样适合评估任务。

**统计鲁棒性与方差分析**：箱线图分布提供了对每个评测模型性能统计鲁棒性的洞察。具有更紧密分布和更少异常值的模型展现出更可预测和稳定的评估行为，这对公平一致的模型比较至关重要。某些评测模型中异常值的存在突出了自动评测可能偏离人类判断的潜在边缘案例，为我们理解评测局限性提供了信息。


### 📈 关键洞察

通过对2023年至2025年期间大语言模型的持续追踪评测，LLMEval-3揭示了模型发展的显著规律。最引人注目的发现是技术进步的加速趋势，模型性能呈现指数级改进模式，从根本上改变了AI能力的发展轨迹。性能指标从GPT-3.5-turbo的51.9分基线跃升至当前最先进的Doubao-1.5-Thinking-Pro的93.67分，在短短两年内实现了绝对意义上80%的性能提升。

我们的分析还发现了多语言能力的显著进步，特别是在中文语言理解和学术知识处理方面。国产中文模型展现出卓越的竞争实力，多个模型在特定学科评测中达到或超越国际基准水平。这一趋势表明本土化AI发展的成熟，以及区域特色语言模型能力的崛起。

评测方法本身也表现出高度的鲁棒性，我们的相对评分系统有效缓解了随机抽样通常引入的系统性偏差。跨时序一致性分析显示，保持较高相对分数的模型在多轮评测中展现出更稳定的性能模式，验证了我们动态基准方法的可靠性。此外，学科专业分析揭示了不同学术领域的独特性能模式，STEM领域相比人文社科通常显示更高的可达成分数，暗示了不同知识领域的复杂度阈值差异。

## 🎓 学术贡献

### 🏅 研究影响

LLMEval-3在大语言模型评测领域做出了开创性贡献，建立了首个系统性追踪多年跨度和多学科领域模型演进的综合纵向基准。我们的工作填补了现有评测方法的关键空白，通过提供持续、标准化的评估协议，实现了对不同时期开发模型的公平比较。这种纵向研究方法产生了前所未有的AI发展轨迹洞察，揭示了通过静态基准评测无法观察到的加速模式和能力涌现趋势。

我们引入的鲁棒相对评测框架构成了方法论创新，解决了跨时序模型比较中的根本公平性问题。通过实施针对当前最先进模型而非固定历史基线的动态基准评测，我们创建了一个随领域发展保持相关性和区分度的评测系统。这种方法在记录近期AI快速发展方面特别有价值，传统固定基准在快速发展的环境中往往很快变得饱和或过时。

我们对自动评估方法的全面验证为大规模AI评测系统的可靠性提供了关键实证证据。通过与人类专家判断的广泛比较，我们证明了精心设计的自动评测可以与人工评估达到高度一致性，同时实现综合模型比较所需的规模。这项验证工作对更广泛的AI评测社区具有重要意义，为未来大规模基准开发提供了方法论基础。

### 🔮 未来方向

- **规模扩展**：从20万题扩展至100万题
- **多模态集成**：文本-图像和数学公式评测
- **全球本地化**：多语言基准版本
- **实时平台**：在线评测和监控系统

## 🚀 快速开始

### 在线评测
访问我们的评测平台：**[http://llmeval.com/](http://llmeval.com/)**

### 机构接入
1. 在我们的平台注册账户
2. 联系管理员进行机构认证
3. 提交模型评测请求
4. 获取详细性能报告

### 评测流程
- **公平抽样**：每个模型接收独特的题目集
- **标准化协议**：一致的评测程序
- **全面报告**：详细的性能分析
- **排行榜集成**：自动排名更新





## 🤝 贡献参与

我们欢迎研究社区的贡献！请随时：

- 提交问题报告或功能请求
- 提出新的评测指标或方法
- 贡献额外的题目集或领域
- 建议改进我们的评测框架

## 📞 联系我们

如有问题、建议或合作机会：

- **网站**：[http://llmeval.com/](http://llmeval.com/)
- **邮箱**：mingzhang23@m.fudan.edu.cn
- **微信**：zanyingluan

---

<div align="center">

**LLMEval-3** | 构建LLM评测的未来

</div>
